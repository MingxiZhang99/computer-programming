{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MingxiZhang99/computer-programming/blob/main/lab2_transformer-final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f0d3a9",
      "metadata": {
        "tags": [],
        "id": "a2f0d3a9"
      },
      "source": [
        "# Deep learning lab 2: Transformers and self-attention\n",
        "\n",
        "The goal of this lab is that you get some hands on experience with the transformer architecture. In the first part of the lab we will work with the self-attention mechanism and implement it in pytorch. For this part the content from the first Lecture on Transformers is sufficient. In the second part we will train two different transformer architectures with different tasks. In the outro session of the lab you are asked to present the solution to the questions of the lab to the teachers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3f63fa22",
      "metadata": {
        "tags": [],
        "id": "3f63fa22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3ac4d1",
      "metadata": {
        "tags": [],
        "id": "7c3ac4d1"
      },
      "source": [
        "An important part of the Transformer architecture is the fully-connected neural network which we have seen in Assignment and Lab 1. Typically, a neural network with one hidden layer is used for this with the hidden dimension being 4 times larger than the input/output dimension. We use the GeLU non-linear activation function, which is a smoothed out version of the ReLU function you have seen in the previous sessions, for the non-linearity as it is the default choice for Transformer layers. By now, you should be familiar with this general architecture and understand the implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1a7e9889",
      "metadata": {
        "tags": [],
        "id": "1a7e9889"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_dimension):\n",
        "        super().__init__()\n",
        "        self.c_fc = torch.nn.Linear(input_dimension, 4 * input_dimension)\n",
        "        self.gelu = torch.nn.GELU(approximate='tanh')\n",
        "        self.c_proj = torch.nn.Linear(4 * input_dimension, input_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06aa1a1d",
      "metadata": {
        "tags": [],
        "id": "06aa1a1d"
      },
      "source": [
        "## Part 1: Self-attention mechanism\n",
        "\n",
        "The self-attention mechanism in matrix form is given by the following equations. For more details you can check Section 12.2.4 in the course book.\n",
        "\n",
        "$$\n",
        "\\mathbf{Sa}[\\mathbf{X}] = \\mathbf{V}[\\mathbf{X}] \\cdot \\mathbf{Softmax} \\left[ \\mathbf{K}[\\mathbf{X}]^T \\mathbf{Q}[\\mathbf{X}] \\right]\n",
        "$$\n",
        "\n",
        "where $\\mathbf{Q[X]}$, $\\mathbf{K[X]}$, $\\mathbf{V[X]}$ are the Key, Query and Value matrices given by linear transformations of the input $x$ to the layer:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{Q}[\\mathbf{X}] = \\mathbf{\\beta_q 1^T + \\Omega_q X} \\\\\n",
        "\\mathbf{K}[\\mathbf{X}] = \\mathbf{\\beta_k 1^T + \\Omega_k X} \\\\\n",
        "\\mathbf{V}[\\mathbf{X}] = \\mathbf{\\beta_v 1^T + \\Omega_v X}\n",
        "\\end{align}\n",
        "\n",
        "Similar to the book, we do not include the dependence of $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ on the input $\\mathbf{X}$ in the following. Note in particular that the input and output dimensions are the same for the layer.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "When implementing the architecture in pytorch, we rely on the transposed version of the self-attention since we additionally have a batch dimension. The input itself is usually of dimension $(B, N, D)$ where $B$ is the batch size, $N$ is the number of tokens / elements in the sequence and $D$ is the dimensionality of the input also refred to as input embedding.\n",
        "In its transposed form, the self-attention mechanism can be written as:\n",
        "\n",
        "$$\n",
        "\\mathbf{Sa}[\\mathcal{X}] = \\mathbf{Softmax} \\left[\\mathcal{Q}[\\mathcal{X}]  \\mathcal{K}[\\mathcal{X}]^T \\right] \\cdot \\mathcal{V}[\\mathcal{X}]\n",
        "$$\n",
        "\n",
        "which is the transposed version of the equation shown above where $\\mathcal{X}$ is the input tensor of size $N \\times D$. The softmax operation above should be applied row-wise in this case. For an efficient implementation, use the transposed formulation above when implementing the modules in the lab.\n",
        "This forumlation also allows us to easily parallelize over multiple batches by including the batch dimension as the first dimension of the input tensor. PyTorch parallelizes over the leading dimensions the standard operations using broadcasting.\n",
        "\n",
        "**Task 1**: In the init method of the single-head self attention, create the parameterized layers that are required to compute the output of the mechanism described above.\n",
        "\n",
        "**Task 2**: Write the forward pass of the module to implement the self-attention mechanism. Return both the output tokens as well as the attention matrix itself as a second return argument.\n",
        "\n",
        "_Hints_:\n",
        "- Learned matrix-vector products can be efficiently implemented using linear layers using torch.nn.Linear.\n",
        "- When using a transformer, the input usually is a tensor of size (batch_size, num_tokens, embedding_size) = $(B, N, D)$ so be careful when transposing tensors. Using the function .transpose() you can pass the dimensions you want to transpose as an input argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "01a84ecb",
      "metadata": {
        "tags": [],
        "id": "01a84ecb"
      },
      "outputs": [],
      "source": [
        "class SingleHeadSelfAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dimension):\n",
        "        super().__init__()\n",
        "        # TODO: implement the key, query and value operators using torch.nn.Linear\n",
        "        self.key = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "        self.query = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "        self.value = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement forward pass here\n",
        "\n",
        "        keys = self.key(x)\n",
        "        queries = self.query(x)\n",
        "        values = self.value(x)\n",
        "        # compute the scaled dot product attention\n",
        "        # attention = softmax(queries @ keys.T / sqrt(embedding_dimension)) @ values\n",
        "        attention = torch.matmul(queries, keys.transpose(1, 2)) / np.sqrt(x.shape[-1])\n",
        "        attention = torch.nn.functional.softmax(attention, dim=-1)\n",
        "        x = torch.matmul(attention, values)\n",
        "        # return the new tokens and the attention matrix\n",
        "        return x, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3dc2f903",
      "metadata": {
        "tags": [],
        "id": "3dc2f903"
      },
      "outputs": [],
      "source": [
        "# Create a single head self-attention layer with embedding size 32\n",
        "sa = SingleHeadSelfAttention(32)\n",
        "\n",
        "# create a normally distributed vectors\n",
        "# vector x is of size (BATCHSIZE, NUM_TOKENS, EMBEDDING_SIZE) = (B, N, D)\n",
        "x = torch.empty((2, 10, 32)).normal_()\n",
        "out, att = sa(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4243efed",
      "metadata": {
        "tags": [],
        "id": "4243efed"
      },
      "source": [
        "**Question 1**: What size is the attention matrix that gets returned by the single-head layer? What constraints must the attention matrix and its elements fulfill in terms of the attention matrix size and the values that the elements in the matrix take?\n",
        "\n",
        "_Hint_: you can look at the following easy checks that verify the constraints of the computed attention matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "af4865da",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af4865da",
        "outputId": "7f889465-6ce6-49f5-da2b-55c5fdcdcedb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# the attention matrix should be of size (B, N, N)\n",
        "att.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "74e8e78d",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74e8e78d",
        "outputId": "f760f0bf-bb6c-4e79-c9dc-138b65a112d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([], dtype=torch.int64),\n",
              " tensor([], dtype=torch.int64),\n",
              " tensor([], dtype=torch.int64))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# all elements in the attention matrix should be non-negative\n",
        "torch.where(att < 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4c615257",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c615257",
        "outputId": "2d356d50-8323-4af0-a038-402a09308c07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000]], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# the rows in the attention matrix should sum to one\n",
        "torch.sum(att, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465877bc",
      "metadata": {
        "tags": [],
        "id": "465877bc"
      },
      "source": [
        "Now, we have managed to implement the heart of the transformer -- the self-attention mechamism. In the following, we will extend the architecture to include normalization, multiple attention heads, and passing the results through non-linear MLPs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda4f387",
      "metadata": {
        "tags": [],
        "id": "cda4f387"
      },
      "source": [
        "In the cell below, you can see a visualization of the attention matrix of the initalized transformer.\n",
        "\n",
        "**Question 2**: What can you observe about the distribution of attention scores? Why is this problem arising and how can we avoid this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a7391ca7",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "a7391ca7",
        "outputId": "b9b00e78-010f-4bea-bb4d-badab203909b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(att[0].detach().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63a6611",
      "metadata": {
        "tags": [],
        "id": "b63a6611"
      },
      "source": [
        "**Task 3**: This scaling problem arises from the fact that the softmax function can quickly focus on a single maximum value, causing all other components to become very small. In order to fix the scaling of the attention matrix and make training easier, add the scaling constant given in the book in equation (12.9) to the forward pass of the single-head self-attention layer. How does the attention map look after normalizing the attention step?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aacee10e",
      "metadata": {
        "tags": [],
        "id": "aacee10e"
      },
      "source": [
        "**Task 4**: In practice, using the single-head self-attention is usually not sufficient to get good performance of the model. Instead, we implement a **multi-head self-attention** next. Therefore, you need to add an additional hyper-parameter to the module which allows us to specify the number of attention heads we want to use.\n",
        "Implement the multi-head attention in the code cell below:\n",
        "\n",
        "_Hints_:\n",
        "- Note that you can use the same parameterization for the multi-head attention as for the single head case in the init function for the query, key and value transformations.\n",
        "- You need to add an additional linear transformation to combine the results from the different heads after the self-attention steps.\n",
        "- After computing the key, queries, and values you should reshape the results in a 4d tensor of size $(B, N, H, D / H)$ where $H$ is the number of heads in your self-attention model.\n",
        "- Since we want to parallelize the computations over the batch dimension and the number of heads, we want to reshape the tensor such that these dimensions are in the front dimensions. Don't forget to reshuffle again before you bring the output in the final form which is the same as the input itself.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e02b39b7",
      "metadata": {
        "tags": [],
        "id": "e02b39b7"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, embedding_dimension):\n",
        "\n",
        "        super().__init__()\n",
        "        assert embedding_dimension % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # TODO: add key, query and value transformations using torch.nn.Linear\n",
        "        # TODO: add the final projection transformation\n",
        "        self.key = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "        self.query = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "        self.value = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "        self.proj = torch.nn.Linear(embedding_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, num_tokens, embedding_dimension)\n",
        "        B, N, D = x.size()\n",
        "        # TODO: compute the key, queries and values for the input then reshape the result and compute the scaled self-attention mechanism\n",
        "        keys = self.key(x)\n",
        "        queries = self.query(x)\n",
        "        values = self.value(x)\n",
        "        keys = keys.view(B, N, self.num_heads, D // self.num_heads).transpose(1, 2)\n",
        "        queries = queries.view(B, N, self.num_heads, D // self.num_heads).transpose(1, 2)\n",
        "        values = values.view(B, N, self.num_heads, D // self.num_heads).transpose(1, 2)\n",
        "        attention = torch.matmul(queries, keys.transpose(2, 3)) / np.sqrt(D // self.num_heads)\n",
        "        attention = torch.nn.functional.softmax(attention, dim=-1)\n",
        "        x = torch.matmul(attention, values)\n",
        "        x = x.transpose(1, 2).reshape(B, N, D)\n",
        "        x = self.proj(x)\n",
        "        att = attention\n",
        "        # return the new tokens and the attention matrix\n",
        "        return x, att\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3e017648",
      "metadata": {
        "tags": [],
        "id": "3e017648"
      },
      "outputs": [],
      "source": [
        "sa = MultiHeadSelfAttention(4, 32)\n",
        "\n",
        "# we create a normally distributed vectors\n",
        "# vector x is of size (BATCHSIZE, NUM_TOKENS, EMBEDDING_DIMENSION) = (B, N, D)\n",
        "x = torch.empty((2, 10, 32)).normal_()\n",
        "out, att = sa(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6fcafcf3",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcafcf3",
        "outputId": "dfcaeb51-0e2a-41c6-ddbf-8848369ebc99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# check the output dimension, expected: (B, N, D)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d7202ef9",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7202ef9",
        "outputId": "5595b3ff-2718-4534-cf26-6a017c943a2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 10, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# check the shape of the attention matrix, expected: (B, H, N, N)\n",
        "att.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50217fd",
      "metadata": {
        "tags": [],
        "id": "c50217fd"
      },
      "source": [
        "Next, we want to combine the just implemented self-attention block with additional non-linearities in order to get a Transformer Block which consists of layer norms, self-attention and a multi-layer perceptron as shown in the Figure below:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "Note that the batch dimension is not shown in this figure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ed1259",
      "metadata": {
        "tags": [],
        "id": "a5ed1259"
      },
      "source": [
        "**Task 5**: Implement the forward pass in the transformer encoder block in the function below. Note that the just implemented attention mechanism returns two arguments the first one being the output of the transformer and the second one the attention matrix that we keep for visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "faefb3f3",
      "metadata": {
        "tags": [],
        "id": "faefb3f3"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(torch.nn.Module):\n",
        "    def __init__(self, num_heads, embedding_dimension):\n",
        "        super().__init__()\n",
        "        self.ln1 = torch.nn.LayerNorm(embedding_dimension)\n",
        "        self.attn = MultiHeadSelfAttention(num_heads, embedding_dimension)\n",
        "        self.ln2 = torch.nn.LayerNorm(embedding_dimension)\n",
        "        self.mlp = MLP(embedding_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        att, att_w = self.attn(x)\n",
        "        # TODO: implement the forward pass and return the output and the attention matrix\n",
        "        x = x + att\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x)\n",
        "        x = self.ln2(x)\n",
        "\n",
        "\n",
        "        return x, att_w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7680936d",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7680936d",
        "outputId": "4df0b4be-0024-4608-826f-26559f5bb439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 32])\n"
          ]
        }
      ],
      "source": [
        "encoder = EncoderBlock(2, 32)\n",
        "\n",
        "x = torch.empty((2, 10, 32)).normal_()\n",
        "out, att = encoder(x)\n",
        "\n",
        "# the output should have the same shape as the input x\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67329d91",
      "metadata": {
        "tags": [],
        "id": "67329d91"
      },
      "source": [
        "**Question 3**: How many parameters does each part of the transformer block have for an embedding dimension of 100?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7513d124",
      "metadata": {
        "tags": [],
        "id": "7513d124"
      },
      "source": [
        "## Part 2a: Sequence-to-sequence task - reversing a list\n",
        "\n",
        "We start with a basic example of a sequence-to-sequence task.\n",
        "Similar problems arise for example in machine translation and text summarization.\n",
        "However, here we consider an even simpler problem of reversing an input sequence.\n",
        "\n",
        "The input consists of $N$ numbers between 0 and $M$, each serving as an input token to the Transformer. We can simply represent each of the inputs as a one-hot encoding of the number using a $M$-dimensional vector. Even though this sounds like a very simple task, RNNs and other architectures can have issues with this due to the long range dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ce861d0b",
      "metadata": {
        "tags": [],
        "id": "ce861d0b"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "from functools import partial\n",
        "\n",
        "class ReverseDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        return inp_data, labels\n",
        "\n",
        "# hyper-parameters for the dataset\n",
        "num_classes = 10\n",
        "sequence_len = 16\n",
        "\n",
        "# creating a dataset\n",
        "dataset = partial(ReverseDataset, num_classes, sequence_len)\n",
        "train_loader = data.DataLoader(dataset(50_000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_loader   = data.DataLoader(dataset(1_000), batch_size=128)\n",
        "test_loader  = data.DataLoader(dataset(10_000), batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e795a8",
      "metadata": {
        "tags": [],
        "id": "99e795a8"
      },
      "source": [
        "Here is an example sequence from the dataset with the correctly reversed labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e07f61bf",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e07f61bf",
        "outputId": "b7a55a7d-f511-42f2-b96f-cebb61783279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data: tensor([4, 4, 2, 9, 8, 1, 5, 3, 8, 7, 7, 7, 1, 2, 1, 9])\n",
            "Labels:     tensor([9, 1, 2, 1, 7, 7, 7, 8, 3, 5, 1, 8, 9, 2, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "inp_data, labels = train_loader.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf7a21df",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "bf7a21df"
      },
      "source": [
        "Next, we create a encoder architecture to reverse the list which simply consists of several encoder blocks implemented in the previous section.\n",
        "\n",
        "**Question 4**: Why are we using an encoder here instead of a decoder or an encoder-decoder architecture?\n",
        "\n",
        "**Task 6**: In addition to the transformer blocks, we need a predictive network that transforms the network output into the predictions. Implement a neural network with one hidden layer with the given hidden dimesion that takes the output from the last encoder block and predicts the token at each position. You can implement this using a sequential list of modules as started in the code below. Aditionally, we need to add a positional encoding for the network to know the position of the elements in the sequence. This is already implemented in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bd4d5943",
      "metadata": {
        "tags": [],
        "id": "bd4d5943"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, sequence_length, num_heads, embedding_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList([EncoderBlock(num_heads, embedding_dimension) for _ in range(num_layers)])\n",
        "        self.sequence_length = sequence_length\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        hidden_dimension = 100\n",
        "        self.predictor = torch.nn.Sequential(\n",
        "            # TODO: write a non-linear prediction network here\n",
        "            nn.Linear(embedding_dimension, hidden_dimension),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dimension, embedding_dimension)\n",
        "        )\n",
        "\n",
        "        # positional encoding\n",
        "        self.pe = torch.nn.Embedding(sequence_length, embedding_dimension)\n",
        "    def forward(self, x):\n",
        "        atts = []\n",
        "\n",
        "        # positional encodings\n",
        "        pos_encodings = self.pe(torch.arange(self.sequence_length).unsqueeze(0))\n",
        "        #print(x.shape, pos_encodings.shape)\n",
        "        x = x + pos_encodings\n",
        "\n",
        "        # forward pass\n",
        "        for l in self.layers:\n",
        "            x, w = l(x)\n",
        "            atts.append(w.detach())\n",
        "\n",
        "        x = self.predictor(x)\n",
        "        return x, atts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764a4bad",
      "metadata": {
        "tags": [],
        "id": "764a4bad"
      },
      "source": [
        "In the function below, we compute the loss function for the task. We use a one-hot encoding for the different numbers in the list and compute the cross entropy loss to classiy the predictions at each position of the reversed list correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "13d40496",
      "metadata": {
        "tags": [],
        "id": "13d40496"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, batch):\n",
        "    inp_data, labels = batch\n",
        "    # forward pass and loss function\n",
        "    inp_data = torch.nn.functional.one_hot(inp_data, num_classes=num_classes).float()\n",
        "    output, _ = model(inp_data)\n",
        "    loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), labels.view(-1))\n",
        "\n",
        "    acc = (output.argmax(dim=-1) == labels).float().mean()\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e22d96",
      "metadata": {
        "tags": [],
        "id": "b8e22d96"
      },
      "source": [
        "Next, we use the loss function to train our model. This training loop follows the standard mechanism you should be familar with at this stage from the previous assignments and labs. Training this model should only take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f4739efd",
      "metadata": {
        "tags": [],
        "id": "f4739efd"
      },
      "outputs": [],
      "source": [
        "# using a one layer with two heads\n",
        "encoder_model = TransformerEncoder(num_layers=1, num_heads=2, embedding_dimension=num_classes, sequence_length=sequence_len)\n",
        "optimizer = torch.optim.AdamW(encoder_model.parameters())\n",
        "num_epochs = 10\n",
        "\n",
        "losses = []\n",
        "accs = []\n",
        "\n",
        "# loop over the epochs and training data\n",
        "for e in range(num_epochs):\n",
        "    encoder_model.train()\n",
        "\n",
        "    for b in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss, acc = compute_loss(encoder_model, b)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # logging\n",
        "        losses.append(loss.item())\n",
        "        accs.append(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f412d0",
      "metadata": {
        "tags": [],
        "id": "67f412d0"
      },
      "source": [
        "Below, we can see the loss function and the accuracy for the training performance of the network.\n",
        "\n",
        "Your final model should reach a performance of 100% on the test data as the considered task is quite easy. You might need to increase the number of heads to get a good performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3bbf612b",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "3bbf612b",
        "outputId": "dc4dc6d9-34b1-412a-9667-643fe2dd9374"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "\n",
        "axes[0].set_title(\"Training loss\")\n",
        "axes[0].plot(losses, alpha=0.5)\n",
        "axes[0].grid()\n",
        "\n",
        "axes[1].set_title(\"Training accuracy\")\n",
        "axes[1].plot(accs, alpha=0.5)\n",
        "axes[1].grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43048eb",
      "metadata": {
        "tags": [],
        "id": "b43048eb"
      },
      "source": [
        "**Question 5**: What happens when removing the positional encoding from the encoder network above with the performance of the network?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "04a98f06",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04a98f06",
        "outputId": "c33189ba-b400-426e-9b63-da300296bd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set performance 0.9999691\n"
          ]
        }
      ],
      "source": [
        "# test the model on the unseen test data\n",
        "test_accs = []\n",
        "for b in test_loader:\n",
        "    loss, acc = compute_loss(encoder_model, b)\n",
        "    test_accs.append(acc)\n",
        "\n",
        "print(\"Test set performance\", np.mean(test_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "67670fc6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67670fc6",
        "outputId": "6ba2b0f6-594c-4124-94d4-a22888546bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6, 6, 6, 7, 1, 2, 8, 9, 5, 3, 9])\n",
            "tensor([9, 3, 5, 9, 8, 2, 1, 7, 6, 6, 6, 5, 4, 3, 2, 1])\n"
          ]
        }
      ],
      "source": [
        "# test sample...\n",
        "test = torch.Tensor([1, 2, 3, 4, 5, 6, 6, 6, 7, 1, 2, 8, 9, 5, 3, 9]).type(torch.LongTensor)\n",
        "print(test)\n",
        "test = torch.nn.functional.one_hot(test, num_classes=num_classes).float()\n",
        "\n",
        "output, att = encoder_model(test)\n",
        "\n",
        "print(torch.argmax(output.squeeze(), dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba28e3c",
      "metadata": {
        "tags": [],
        "id": "dba28e3c"
      },
      "source": [
        "In the cell below, you can see a visualization of the two attention matrices of the transformer encoder.\n",
        "\n",
        "**Question 6**: Can you intepret the pattern in these attention matrices?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "732b0d40",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "732b0d40",
        "outputId": "72b296c8-ead7-428b-d9f3-88e7071ff0b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# visualize the attention maps\n",
        "att_maps = att[0]\n",
        "\n",
        "plt.imshow(att[0][0, 0].detach().numpy())\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(att[0][0, 1].detach().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c41f4fc",
      "metadata": {
        "tags": [],
        "id": "9c41f4fc"
      },
      "source": [
        "## Part 2b: Sequence generation task\n",
        "\n",
        "In the second example, we train a decoder model to generate text. In this part of the lab we are using the tiny sheakspear dataset. It is a small dataset containing the plays by Shakespear. First, we need to obtain the data and read it in. We use every single character as an individual token. Therefore, we first find out the number of distinct characters in all the plays and then create a mapping from each character to a unique number.\n",
        "\n",
        "For this task it is also nice to use a GPU e.g. on google colab to get a faster training. However, the model can also be trained locally on a CPU and the training time should still be relativly fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bc6d5023",
      "metadata": {
        "id": "bc6d5023"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7c44f3b5-41d1-4828-bebf-b09cb2073af4",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "7c44f3b5-41d1-4828-bebf-b09cb2073af4"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\" # TODO: you can change the device to cuda if you are using colab or a local graphics card."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04498ef-bcf6-4d25-aa14-d58b4ac72fcb",
      "metadata": {
        "tags": [],
        "id": "e04498ef-bcf6-4d25-aa14-d58b4ac72fcb"
      },
      "source": [
        "Let's start by loading the data. Quickly scan the code below and verify that the functions implement the described behaviour above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f56ef695",
      "metadata": {
        "editable": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f56ef695",
        "outputId": "a773e2c6-4f1a-4153-fe3c-5754090ae2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-03 02:44:49--  https://raw.githubusercontent.com/uu-sml/course-dl-public/refs/heads/main/Lab2/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: input.txt\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-03-03 02:44:49 (25.5 MB/s) - input.txt saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load the dataset\n",
        "! wget https://raw.githubusercontent.com/uu-sml/course-dl-public/refs/heads/main/Lab2/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0d934496",
      "metadata": {
        "tags": [],
        "id": "0d934496"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "871cba81",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "871cba81",
        "outputId": "70b4e8c6-2b24-4b68-a009-999cde0e715d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# let's take a quick look at the text :)\n",
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4a4c6849",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a4c6849",
        "outputId": "f0af39a7-d4d3-4932-cae6-92083fbc331c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using vocab size 65\n"
          ]
        }
      ],
      "source": [
        "# we do character level encoding here:\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Using vocab size {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ef100d84",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef100d84",
        "outputId": "0be008ae-3a26-4738-924d-bcb473b6533f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ],
      "source": [
        "# Create a mapping from characters to integers and back.\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # Encoder: Take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: take a list of integers, output a string\n",
        "\n",
        "print(itos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a07dc4",
      "metadata": {
        "tags": [],
        "id": "40a07dc4"
      },
      "source": [
        "Now we can get started to implement a decoder model that we can then use for autoregressive sampling to generate new text sequences.\n",
        "\n",
        "To accomplish this, we first have to adjust the self attention module from above to use causal self attention i.e. only attend to tokens that are ahead in the sequence to them to allow us to efficiently train the model.\n",
        "\n",
        "To get started on this task, copy the code for the multi-head attention implementation in the cell blow. When computing the self-attention scores, we now want to make sure that each token only attends to tokens that come before the token itself in the sequence.\n",
        "This can be achieved by applying a triangular masking to the attention weights.\n",
        "Remember to apply the triangular masking on the unnormalized attention weights before applying the softmax function in order to make sure the columns normalize to one in the attention matrix.\n",
        "\n",
        "**Task 7**: Implement these changes by adding the masking of the self attention matrix in the forward pass in the new class below that we use for causal self-attention in the decoder that we impelment in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "665cb0c1",
      "metadata": {
        "tags": [],
        "id": "665cb0c1"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCausalSelfAttention(torch.nn.Module):\n",
        "    def __init__(self, num_heads, embedding_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure the embedding dimension is divisible by the number of heads\n",
        "        assert embedding_dimension % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.head_dim = embedding_dimension // num_heads\n",
        "\n",
        "        # Linear transformations for query, key, and value\n",
        "        self.W_q = torch.nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "        self.W_k = torch.nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "        self.W_v = torch.nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "        # Final linear transformation to combine the attention heads' outputs\n",
        "        self.W_o = torch.nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
        "\n",
        "        # Softmax for attention distribution\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for multi-head self-attention with masking\n",
        "\n",
        "        Args:\n",
        "            x: (B, N, D)  Input sequence (batch_size, num_tokens, embedding_dimension)\n",
        "\n",
        "        Returns:\n",
        "            output: (B, N, D)  Updated representations after attention\n",
        "            attention: (B, N, N)  Attention matrix showing the attention weights\n",
        "        \"\"\"\n",
        "        B, N, D = x.size()\n",
        "\n",
        "        # Compute Q, K, V for all heads\n",
        "        Q = self.W_q(x)  # (B, N, D)\n",
        "        K = self.W_k(x)  # (B, N, D)\n",
        "        V = self.W_v(x)  # (B, N, D)\n",
        "\n",
        "        # Reshape Q, K, V to (B, N, H, D/H) where H = num_heads\n",
        "        Q = Q.view(B, N, self.num_heads, self.head_dim)  # (B, N, H, D/H)\n",
        "        K = K.view(B, N, self.num_heads, self.head_dim)  # (B, N, H, D/H)\n",
        "        V = V.view(B, N, self.num_heads, self.head_dim)  # (B, N, H, D/H)\n",
        "\n",
        "        # Transpose to get dimensions (B, H, N, D/H)\n",
        "        Q = Q.transpose(1, 2)  # (B, H, N, D/H)\n",
        "        K = K.transpose(1, 2)  # (B, H, N, D/H)\n",
        "        V = V.transpose(1, 2)  # (B, H, N, D/H)\n",
        "\n",
        "        # Compute attention scores (scaled dot-product)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # (B, H, N, N)\n",
        "\n",
        "        # Create a causal mask (upper triangular matrix to prevent attending to future tokens)\n",
        "        mask = torch.triu(torch.ones(N, N), diagonal=1).bool().to(x.device)  # Upper triangular mask (N x N)\n",
        "\n",
        "        # Apply the mask by setting the upper triangular values to a large negative number\n",
        "        attention_scores = attention_scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        # Apply Softmax to normalize the attention scores\n",
        "        attention_matrix = self.softmax(attention_scores)  # (B, H, N, N)\n",
        "\n",
        "        # Compute weighted values using attention matrix\n",
        "        output = torch.matmul(attention_matrix, V)  # (B, H, N, D/H)\n",
        "\n",
        "        # Reshape output back to (B, N, D)\n",
        "        output = output.transpose(1, 2).contiguous().view(B, N, D)  # (B, N, D)\n",
        "\n",
        "        # Final linear transformation to project the concatenated heads back to the original embedding dimension\n",
        "        output = self.W_o(output)  # (B, N, D)\n",
        "\n",
        "        return output, attention_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf72221",
      "metadata": {
        "tags": [],
        "id": "3bf72221"
      },
      "source": [
        "A decoder block and Transformer decoder are very similar to the encoder block and the architecture but utilizes causal self-attention instead. However, we need to make some small adjustments to the task in order to get a good performance:\n",
        "- Instead of using a one-hot encodings for the tokens here, we learn an embedding for each of the 65 tokens to a vector of the embedding dimension that we can choose as a hyper-parameter. For both the positional encoding and the token embedding you should now have a torch.nn.Embedding module.\n",
        "- The final predictive networks also needs adjustment where we want to increase the hidden dimension and choose the correct output dimension. For the hidden dimension you can choose for example 4 times the embedding dimension of the network. The output dimension is equal to the number of avialble classes.\n",
        "\n",
        "**Task 8**: Implement the changes for the decoder architecture below, for this you only need to swap out the full attention in the encoder block with the masked attention module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "88d44946",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "88d44946"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, num_heads, embedding_dimension):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embedding_dimension)\n",
        "        self.attn = MultiHeadCausalSelfAttention(num_heads, embedding_dimension)\n",
        "        self.ln2 = nn.LayerNorm(embedding_dimension)\n",
        "        self.mlp = MLP(embedding_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "        att, att_w = self.attn(x)\n",
        "        x = x + att\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x)\n",
        "        x = self.ln2(x)\n",
        "        return x, att_w\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, embedding_dimension, sequence_length, vocab_size=65):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dimension)\n",
        "        self.position_embedding = nn.Embedding(sequence_length, embedding_dimension)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderBlock(num_heads, embedding_dimension) for _ in range(num_layers)])\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        hidden_dimension = 4 * embedding_dimension\n",
        "\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(embedding_dimension, hidden_dimension),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dimension, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        atts = []\n",
        "\n",
        "        # Token Embedding\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Positional Encoding\n",
        "        sequence_length = x.size(1)\n",
        "        positions = torch.arange(sequence_length, device=x.device).unsqueeze(0).expand(x.size(0), sequence_length)\n",
        "        x = x + self.position_embedding(positions)\n",
        "\n",
        "        # Decoder Blocks\n",
        "        for layer in self.layers:\n",
        "            x, w = layer(x)\n",
        "            atts.append(w.detach())\n",
        "\n",
        "        # Final prediction layer\n",
        "        x = self.predictor(x)\n",
        "        return x, atts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb90fee",
      "metadata": {
        "tags": [],
        "id": "edb90fee"
      },
      "source": [
        "Now, we encode the loaded text data into numerical values and split it into training and test sets. We also implement a function to generate batches from the text data. In decoder models, the task is to predict the next token (in this case, a character) for each input. Therefore, the batch function returns the input shifted one position to the right as the target output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0ecc7d24",
      "metadata": {
        "tags": [],
        "id": "0ecc7d24"
      },
      "outputs": [],
      "source": [
        "# Encode the data to numerical values\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# First 90% will be train, rest val\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# hyper-parameters\n",
        "block_size = 128\n",
        "batch_size = 64\n",
        "\n",
        "# get a batch of data\n",
        "def get_batch(split):\n",
        "    # Generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5cbf34",
      "metadata": {
        "tags": [],
        "id": "6f5cbf34"
      },
      "source": [
        "After the model is trained, we want to sample from it in an autoregressive manner. That means we input a sequence of tokens into the model and compute the probability distribution over the next possible token by the network output. Then we append the newly generated token to the input and repeat this sampling process for a fixed number of steps.\n",
        "\n",
        "**Task 9**: Implement the sampling in the method below. As the next token, choose the output that is predicted to be most likely by the network.\n",
        "\n",
        "_Feel free to change the default prompt to something more exciting._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d8d010c0",
      "metadata": {
        "tags": [],
        "id": "d8d010c0"
      },
      "outputs": [],
      "source": [
        "def sample(model, sample_length=100, prompt=\"The\"):\n",
        "    # transform the prompt to a list of indices\n",
        "    encoded_prompt = encode(prompt)\n",
        "    prompt_tensor = torch.tensor(encoded_prompt, dtype=torch.long).reshape(1, -1).to(device)\n",
        "    generated_sequence = prompt_tensor\n",
        "\n",
        "    for _ in range(sample_length):\n",
        "        # Sample the next token predicted from the decoder network.\n",
        "        next_token_logits,_= model(generated_sequence)\n",
        "\n",
        "        # Get the last token's logits\n",
        "        next_token_logits = next_token_logits[:, -1, :]\n",
        "\n",
        "        # Sample the next token from the logits\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1).reshape(1, 1)\n",
        "\n",
        "        # Add the new sample to the prompt\n",
        "        generated_sequence = torch.cat((generated_sequence, next_token), dim=1)\n",
        "\n",
        "\n",
        "    # Decode the generated sequence and print it\n",
        "    generated_sequence = generated_sequence.squeeze().tolist()\n",
        "    t = decode(generated_sequence)\n",
        "    print(t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4cd363a",
      "metadata": {
        "tags": [],
        "id": "e4cd363a"
      },
      "source": [
        "We can check if the sampling function works in principal by creating a model and sampling from the output. Since the model is not trained yet, the output will be just a collection of random letters here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "671e62ba",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "671e62ba",
        "outputId": "68975e9e-a756-43fa-ae68-1ea6b8d18b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TheygFV!PttWTDtDLLbtFFn\n"
          ]
        }
      ],
      "source": [
        "# sequence length is the maximum length an input sequence can have.\n",
        "transformer = TransformerDecoder(num_layers=3, num_heads=8, embedding_dimension=512, sequence_length=256)\n",
        "#print(transformer)\n",
        "transformer.to(device)\n",
        "# check if the model forward pass and sampling\n",
        "sample(transformer, sample_length=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f1ba546",
      "metadata": {
        "tags": [],
        "id": "3f1ba546"
      },
      "source": [
        "Now, we can start training our model using the code below.\n",
        "\n",
        "_Note_: You can also change the network architecture but bigger models are slower to train and require more compute. Using the standard architecture outlined here, the training takes around 20 minutes on my laptop without any GPU access. You should see a significant improvement over the random sampling in the model but the output might not make a lot of sense due to the small scale of the computations used here. Training the model longer is not required to complete the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4887159",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4887159",
        "outputId": "d4a2946e-b8b2-425a-ee96-f405f8053185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\t loss: 4.23\n",
            "The       t                        t  t t t            t  t    t     t     t        t        t t       \n",
            "----\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(transformer.parameters())\n",
        "\n",
        "max_iter = 5000\n",
        "losses = []\n",
        "\n",
        "eval_freq = 100\n",
        "\n",
        "for i in range(max_iter):\n",
        "\n",
        "    # get the training datta\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # compute the predictions\n",
        "    pred,_ = transformer(xb)\n",
        "\n",
        "    # reshape the output and target and compute the loss\n",
        "    B, N, D = pred.shape\n",
        "\n",
        "    logits = pred.view(B*N, D)\n",
        "    targets = yb.view(B*N)\n",
        "\n",
        "    #print(logits)\n",
        "\n",
        "    loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "    losses.append(loss.detach().item())\n",
        "\n",
        "    # compute the backward pass and update parameters\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # do evaluation\n",
        "    if i % eval_freq == 0:\n",
        "        print(f\"Iteration: {i}\\t loss: {loss.item():.2f}\")\n",
        "        sample(transformer)\n",
        "        print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e83a2b",
      "metadata": {
        "tags": [],
        "id": "18e83a2b"
      },
      "source": [
        "Next, we can visualize the loss over the iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1277f05f",
      "metadata": {
        "tags": [],
        "id": "1277f05f"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e9c7fad",
      "metadata": {
        "tags": [],
        "id": "1e9c7fad"
      },
      "source": [
        "Now that our model is trained, you can again draw samples from the model. You can also change the initial prompt to get different starting points for the sample drawing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1fba79",
      "metadata": {
        "tags": [],
        "id": "3d1fba79"
      },
      "outputs": [],
      "source": [
        "# draw samples from the trained model\n",
        "prompt = \"The\"\n",
        "sample(transformer, prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec82583",
      "metadata": {
        "tags": [],
        "id": "1ec82583"
      },
      "source": [
        "__Question 7__: Using the current sampling strategy, the network output is deterministic. How could you change the sampling such that the network can produce multiple output sequences for a single input? (_You do not have to implement these changes_)\n",
        "\n",
        "Note that we use a very small language model here. If you want to get some better results you would need a larger model which requires more training. However, the implementation you have done here basically includes all the components that the GPT and similar model families include. By simply changing the hyper-parameters you can reimplement GPT-2 in this lab (which is the last model in the GPT architecture where the model is well described in a research paper).\n",
        "\n",
        "More modern models add a lot of computational tricks in order to make the model more scalable and so on but the underlying principles are the same as the ones you have implemented yourself in this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7776db4",
      "metadata": {
        "tags": [],
        "id": "b7776db4"
      },
      "source": [
        "## Summary & Outlook\n",
        "\n",
        "In this lab you have seen how to implement a transformer using a single head or multiple heads either using a decoder style model for sequence-to-sequence tasks and a small example for a decoder model. The code you have written includes everything the basic GPT models consist of."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538ca7df",
      "metadata": {
        "tags": [],
        "id": "538ca7df"
      },
      "source": [
        "Hopefully, this lab has given you some hands on experience with the transformer architecture to understand what is going on under the hood of these models that run here.\n",
        "\n",
        "The examples shown here are toy examples that show the potential power of transformers but are in themselves fairly easy. However, transformers nowadays are capable of remarkable performances. While this course covers the basics for the transformer architecture, you will see similar models again in follow up courses:\n",
        "\n",
        "- _Large Language Models and Societal Consequences of Artificial Intelligence_ where the focus will be on building an LLM application, leveraging transformers for text and language, and exploring their impact. Through the assignments, you will work with state-of-the-art LLMs.\n",
        "- _Advanced Deep Learning for Image Processing_ is a course focusing on deep learning models for images and also covers the suprising competetivness of transformers on image data called vision transformers (ViT)."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "d2l_copy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}